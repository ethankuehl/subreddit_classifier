{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "As of 2016, [roughly one third](https://www.pewresearch.org/internet/2016/10/25/political-content-on-social-media/) of Americans comment, discuss, or post about politics on social media. As a result, political campaigns have begun to invest significant resources into political advertising on social media. In order to craft targeted ads, political advertising agencies are investing significant resources into [identifying the populations of social media users discussing politics](https://www.americanbar.org/groups/crsj/publications/human_rights_magazine_home/voting-in-2020/political-advertising-on-social-media-platforms/). The goal of this project is to build a text classification model that can differentiate between casual and political conversations on social media to aid advertising agencies target particular users for specific ads. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "My models are trained using text data from the website reddit, with the submissions specifically coming from the subreddits: \n",
    "1. r/CasualConversation\n",
    "2. r/PoliticalDiscussion\n",
    "\n",
    "The CasualConversation subreddit is a forum dedicated having fun conversations \"about anything that is on your mind\". with 1.44 million members, there is a huge variety in conversation topics from advice, to discussing dinner plans, favorite memories. As a result, it serves as a great baseline signal for what average conversation looks like on social media.\n",
    "\n",
    "The PoliticalDiscussion subreddit is a forum focused solely on posing questions regarding current politics, mainly centering on US politics as its core topic. The subreddit is home to 1.91 million redditors who have vigorous debates regarding political strategy and opinions on recent political news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "\n",
    "* Collect data from 10,000 posts from r/CasualConversations and r/PoliticalDiscussions using Pushshift API.\n",
    "    * I collect an even number of posts from each subreddit so that the baseline accuracy is equivalent to the flip of a coin: 50/50. \n",
    "* Clean text data, engineer new features with NLP, and lemmatize text so that each word maintains its meaning but is reduced to its base form.  \n",
    "* Split data into training and testing datasets to validate the performance of my model. Model will be created with training data, and then its accuracy will be tested using the testing data. \n",
    "* Vectorize the text data using TF-IDF methodology to account for outliers and weigh each word according to its importance to the meaning of a senetence. \n",
    "* Fit the data to a logistic regression and random forest classification model. Analyze the classification metrics to determine which model performed better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "### Reddit NLP - Data Collection\n",
    "\n",
    "[Pushshift](https://github.com/pushshift/api) is a service created y the /r/datasets mod team to help provide enhanced search capapbilities for searching Reddit data. The Pushshift RESTful API allows for a higher level search functionality and querying of comments and submissions, aiding in data collection for analysis and modeling. The API leverages the requests library to return a json response that can then be parsed for the data of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T09:49:58.971155Z",
     "start_time": "2021-01-29T09:49:58.961324Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T09:49:58.980060Z",
     "start_time": "2021-01-29T09:49:58.974757Z"
    }
   },
   "source": [
    "### Query Syntax\n",
    "\n",
    "Setting the query url to the Pushshift API for selecting subreddit submissions. In the context of my subreddits, the submissions themselves are more verbose and provide a greater indication of the topic of conversation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T09:49:58.987087Z",
     "start_time": "2021-01-29T09:49:58.983969Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am setting the query parameters, specifying the subreddits I want to collects submissions from and number of submissions I want to collect for each query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T09:49:58.995488Z",
     "start_time": "2021-01-29T09:49:58.990213Z"
    }
   },
   "outputs": [],
   "source": [
    "casual_params = {\n",
    "    'subreddit': 'casualconversation',\n",
    "    'size': 100\n",
    "}\n",
    "\n",
    "political_params = {\n",
    "    'subreddit': 'politicaldiscussion',\n",
    "    'size': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_1 = requests.get(url, casual_params)\n",
    "print(response_1.status_code)\n",
    "\n",
    "response_2 = requests.get(url, political_params)\n",
    "print(response_2.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The status code of 200 tells us that the query was accepted for both subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the Subreddits\n",
    "\n",
    "In order to collect the most text per post in each subreddit, I wrote a function that scrapes each subreddit for posts that have not been removed, deleted, or have empty post text. By doing so, I am ensuring that I have the most text dense posts possible for training my models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T07:47:13.073586Z",
     "start_time": "2021-02-01T07:47:12.595131Z"
    }
   },
   "outputs": [],
   "source": [
    "def scraper(url, api_params):\n",
    "    \n",
    "    '''Requests subreddit submission data from Pushshift API. \n",
    "    ---\n",
    "    Returns:\n",
    "    type: DataFrame\n",
    "        Composed of post submission data. Only includes submissions \n",
    "        that were not removed, deleted, or had empty text posts. \n",
    "    \n",
    "    ---\n",
    "    Parameters:\n",
    "    url\n",
    "        Type: String. \n",
    "        Base Pushshift API url\n",
    "    \n",
    "    api_params:\n",
    "        type: Dictionary. \n",
    "        Specific subreddit querying paramters. \n",
    "    '''\n",
    "    # iterator is 0 before loop\n",
    "    i = 0\n",
    "    \n",
    "    # Append new paramter in loop, so instantiate a fresh dictionary \n",
    "    new_params = api_params\n",
    "    df_list = []\n",
    "    \n",
    "    # Iterating over subreddit submission data until 10,000 posts collected\n",
    "    while i < 10_000:\n",
    "        \n",
    "        # Request with subreddit specific parameters\n",
    "        res = requests.get(url, new_params)\n",
    "        \n",
    "        # Collecting request data \n",
    "        data = res.json()\n",
    "        \n",
    "        # Gathering submission-specific data\n",
    "        posts = data['data']\n",
    "        \n",
    "        # Creating dataframe of all post data from query\n",
    "        df = pd.DataFrame(posts)\n",
    "        \n",
    "        # Iterating over post data from this query\n",
    "        for row in df.index.to_list():\n",
    "            \n",
    "            # Removing data for posts that were removed or deleted\n",
    "            if df.loc[row,'selftext']=='[removed]' or df.loc[row,'selftext']=='[deleted]':\n",
    "                df.drop(row, inplace=True)\n",
    "                continue\n",
    "                \n",
    "            # Removing posts that were empty\n",
    "            if df.loc[row, 'selftext'] == '' or df.loc[row, 'selftext'] == '.':\n",
    "                df.drop(row, inplace=True)\n",
    "        \n",
    "        # Dropping null posts with null text values\n",
    "        df.dropna(subset=['selftext'],inplace=True)\n",
    "            \n",
    "        # Instantiating new query parameter to gather older posts in next query\n",
    "        new_params['before'] = df.iloc[-1]['created_utc']\n",
    "        \n",
    "        # Appending remaining query data to list of DataFrames\n",
    "        df_list.append(df)\n",
    "        \n",
    "        # Progressing iterator forward proportional to the length of data queried\n",
    "        i += len(df)\n",
    "        \n",
    "        # Limiting number of requests per second\n",
    "        time.sleep(2)\n",
    "        \n",
    "    # Concatenating dataframes of all the query data\n",
    "    return pd.concat(df_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T07:47:13.477737Z",
     "start_time": "2021-02-01T07:47:13.472480Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scraping submissions in CasualConversation subreddit and storing in one df\n",
    "casual_df = scraper(url, casual_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T07:47:14.750841Z",
     "start_time": "2021-02-01T07:47:14.746463Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scraping submissions in PoliticalDiscussion subreddit and storing in one df\n",
    "political_df = scraper(url, political_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T11:03:20.228275Z",
     "start_time": "2021-01-29T11:03:19.340882Z"
    },
    "code_folding": []
   },
   "source": [
    "Converting PoliticalDiscussion dataframe into csv \\\n",
    "`political_df.to_csv('./Data/politics.csv', index=False)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T11:03:21.181501Z",
     "start_time": "2021-01-29T11:03:20.232489Z"
    }
   },
   "source": [
    "Converting CasualConversation dataframe into csv\\\n",
    "`casual_df.to_csv('./Data/casual_convo.csv', index=False)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please continue to Notebook 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
