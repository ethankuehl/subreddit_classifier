{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:53.368114Z",
     "start_time": "2021-01-29T20:21:49.431447Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "#import texthero as hero\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:53.827434Z",
     "start_time": "2021-01-29T20:21:53.370799Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/casual_political.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:53.851544Z",
     "start_time": "2021-01-29T20:21:53.845569Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48842639/removing-chinese-in-pandas\n",
    "\n",
    "printable = set(string.printable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:56.869365Z",
     "start_time": "2021-01-29T20:21:53.870757Z"
    }
   },
   "outputs": [],
   "source": [
    "df['selftext'] = df['selftext'].apply(lambda row: ''.join(filter(lambda x: x in printable, row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:56.895355Z",
     "start_time": "2021-01-29T20:21:56.885948Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec_text = CountVectorizer(stop_words='english', analyzer='word', ngram_range=(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:56.913939Z",
     "start_time": "2021-01-29T20:21:56.905149Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec_title = CountVectorizer(stop_words='english', analyzer='word', ngram_range=(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.158957Z",
     "start_time": "2021-01-29T20:21:49.419Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec_title.fit(df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.174055Z",
     "start_time": "2021-01-29T20:21:49.421Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec_text_df = pd.DataFrame(cvec_text.transform(df['selftext']).todense(), \n",
    "                       columns=cvec_text.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.182239Z",
     "start_time": "2021-01-29T20:21:49.423Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.184909Z",
     "start_time": "2021-01-29T20:21:49.425Z"
    }
   },
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(stop_words='english')\n",
    "tvec.fit(df['selftext'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.189363Z",
     "start_time": "2021-01-29T20:21:49.429Z"
    }
   },
   "outputs": [],
   "source": [
    "tvec_df  = pd.DataFrame(tvec.transform(df['selftext']).todense(),\n",
    "                   columns=tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.194282Z",
     "start_time": "2021-01-29T20:21:49.431Z"
    }
   },
   "outputs": [],
   "source": [
    "tvec_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.198303Z",
     "start_time": "2021-01-29T20:21:49.440Z"
    }
   },
   "outputs": [],
   "source": [
    "casual = pd.read_csv('./Data/casual_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.202107Z",
     "start_time": "2021-01-29T20:21:49.442Z"
    }
   },
   "outputs": [],
   "source": [
    "political = pd.read_csv('./Data/political_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.205360Z",
     "start_time": "2021-01-29T20:21:49.443Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48842639/removing-chinese-in-pandas\n",
    "printable = set(string.printable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.208364Z",
     "start_time": "2021-01-29T20:21:49.445Z"
    }
   },
   "outputs": [],
   "source": [
    "casual['text'] = casual['selftext'] + casual['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.214418Z",
     "start_time": "2021-01-29T20:21:49.446Z"
    }
   },
   "outputs": [],
   "source": [
    "political['text'] = political['selftext'] + political['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.217453Z",
     "start_time": "2021-01-29T20:21:49.448Z"
    }
   },
   "outputs": [],
   "source": [
    "casual['text'] = casual['text'].apply(lambda row: ''.join(filter(lambda x: x in printable, row)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.223537Z",
     "start_time": "2021-01-29T20:21:49.513Z"
    }
   },
   "outputs": [],
   "source": [
    "political['text'] = political['text'].apply(lambda row: ''.join(filter(lambda x: x in printable, row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.226382Z",
     "start_time": "2021-01-29T20:21:49.515Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec_casual = CountVectorizer(stop_words='english', analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.230721Z",
     "start_time": "2021-01-29T20:21:49.520Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec_political = CountVectorizer(stop_words='english', analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.247479Z",
     "start_time": "2021-01-29T20:21:49.528Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec_casual.fit(casual['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.251150Z",
     "start_time": "2021-01-29T20:21:49.533Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec_political.fit(political['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.258152Z",
     "start_time": "2021-01-29T20:21:49.539Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec_casual_df = pd.DataFrame(cvec_casual.transform(casual['text']).todense(), \n",
    "                       columns=cvec_casual.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.265468Z",
     "start_time": "2021-01-29T20:21:49.549Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec_casual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.269163Z",
     "start_time": "2021-01-29T20:21:49.553Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec_political_df = pd.DataFrame(cvec_political.transform(political['text']).todense(), \n",
    "                       columns=cvec_political.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.274879Z",
     "start_time": "2021-01-29T20:21:49.560Z"
    }
   },
   "outputs": [],
   "source": [
    "#drop_words = ['com', 'www', 'https', 'http'] # Keeping https, http"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.280527Z",
     "start_time": "2021-01-29T20:21:49.567Z"
    }
   },
   "outputs": [],
   "source": [
    "#cvec_casual_df.drop(drop_words, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.282988Z",
     "start_time": "2021-01-29T20:21:49.570Z"
    }
   },
   "outputs": [],
   "source": [
    "#cvec_political_df.drop(drop_words, inplace=True)\n",
    "cvec_political_df.drop(['politics', 'political', 'politic'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.288492Z",
     "start_time": "2021-01-29T20:21:49.573Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec_political_df['unique_word_count'] = cvec_political_df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.291549Z",
     "start_time": "2021-01-29T20:21:49.579Z"
    }
   },
   "outputs": [],
   "source": [
    "most_common_political = cvec_political_df['unique_word_count'].sort_values(ascending=False).head(n=30).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.296081Z",
     "start_time": "2021-01-29T20:21:49.588Z"
    }
   },
   "outputs": [],
   "source": [
    "most_common_political"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.300393Z",
     "start_time": "2021-01-29T20:21:49.600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Need to filter out links\n",
    "# Need to filter out words with political lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.304041Z",
     "start_time": "2021-01-29T20:21:49.609Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec_casual_df['unique_word_count'] = cvec_casual_df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.308502Z",
     "start_time": "2021-01-29T20:21:49.613Z"
    }
   },
   "outputs": [],
   "source": [
    "most_common_casual = cvec_casual_df['unique_word_count'].sort_values(ascending=False).head(n=30).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.311573Z",
     "start_time": "2021-01-29T20:21:49.632Z"
    }
   },
   "outputs": [],
   "source": [
    "most_common_casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.314371Z",
     "start_time": "2021-01-29T20:21:49.636Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "\n",
    "\n",
    "# Lemmatizing\n",
    "casual['Lemmatize'] = casual['text'].apply(lambda x: lemmatize_sentence(x))\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.321505Z",
     "start_time": "2021-01-29T20:21:49.645Z"
    }
   },
   "outputs": [],
   "source": [
    "casual['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.324615Z",
     "start_time": "2021-01-29T20:21:49.651Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec_casual = CountVectorizer(stop_words='english', analyzer='word', ngram_range=(2,3))\n",
    "cvec_casual.fit(casual['Lemmatize'])\n",
    "\n",
    "cvec_casual_df = pd.DataFrame(cvec_casual.transform(casual['Lemmatize']).todense(), \n",
    "                       columns=cvec_casual.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.330250Z",
     "start_time": "2021-01-29T20:21:49.670Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec_casual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.332393Z",
     "start_time": "2021-01-29T20:21:49.678Z"
    }
   },
   "outputs": [],
   "source": [
    "# tvec = TfidfVectorizer(stop_words='english')\n",
    "# x=tvec.fit_transform(casual['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T20:21:57.336451Z",
     "start_time": "2021-01-29T20:21:49.680Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_1  = pd.DataFrame(tvec.transform(casual['text']).todense(),\n",
    "#                    columns=tvec.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
